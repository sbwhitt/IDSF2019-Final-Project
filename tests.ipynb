{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "df = pd.read_pickle(\"./data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nickruspantini/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Get numerical features with Natural Language Toolkit\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lyrics = df['lyrics']\n",
    "sWords = stopwords.words('english')\n",
    "sWords.extend(('got', 'get', 'gets' 'getting', '2X', '2x', 'x2', 'x3', 'x4', 'x2chorus', 'chorus', 'verse', 'bridge', 'd\\xe3', 'n\\xe3', 'm\\xe3', 'the', 'it', 'is', \"it's\", 'are', 'were', 'a', 'an', 'its', 'of', 'for'))\n",
    "\n",
    "tfidfconverter = TfidfVectorizer(norm='l2', use_idf=True, smooth_idf=True, ngram_range = (1,2), max_features=1000, min_df=3, max_df=0.7, stop_words=sWords)\n",
    "n_features = tfidfconverter.fit_transform(lyrics)\n",
    "df['n_features'] = list(n_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      term         rank\n",
      "507   love  9603.486619\n",
      "438   know  8380.652668\n",
      "475   like  8303.497004\n",
      "607     oh  6499.606956\n",
      "615    one  6374.867464\n",
      "874   time  6301.769009\n",
      "585  never  6137.866518\n",
      "731    see  6061.003503\n",
      "327     go  5793.165852\n",
      "933   want  5537.147598\n"
     ]
    }
   ],
   "source": [
    "# Get top ranking words\n",
    "features = (tfidfconverter.get_feature_names()) \n",
    "sums = n_features.sum(axis = 0) \n",
    "data = [] \n",
    "for col, term in enumerate(features): \n",
    "    data.append( (term, sums[0, col] )) \n",
    "ranking = pd.DataFrame(data, columns = ['term', 'rank']) \n",
    "words = (ranking.sort_values('rank', ascending = False)) \n",
    "print (words.head(10))\n",
    "#print tfidfconverter.get_stop_words()\n",
    "#print tfidfconverter.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an input for X below to test with classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: Bag of words model with polarity scores, word count\n",
    "def f(row):    \n",
    "    np.append(row[5], row[0])\n",
    "    np.append(row[5], row[1])\n",
    "    np.append(row[5], row[2])\n",
    "    np.append(row[5], row[3])\n",
    "    return np.append(row[5], row[4])\n",
    "\n",
    "X = np.vstack((df['pos_score'], df['neg_score'], df['neu_score'], df['compound_score'], df['word_count'], df['n_features'])).T\n",
    "X = np.array(map(f, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: Bag of words model\n",
    "X = n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: Polarity scores, word count\n",
    "X = df[['pos_score', 'neg_score', 'neu_score', 'compound_score', 'word_count']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: Polarity scores\n",
    "X = df[['pos_score', 'neg_score', 'neu_score', 'compound_score']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Score:  0.6352110204439765\n",
      "Time Elapsed:  44.796710968\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  292     9    57    37   230  2681]\n",
      " [    6   119    95     7   134  1068]\n",
      " [   15     8  3292    17   454  1807]\n",
      " [   37     6    56   306   137   999]\n",
      " [   91    44   557    71  1850  5274]\n",
      " [  247    65   717   133  1571 23099]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Country/Folk       0.42      0.09      0.15      3306\n",
      "  Electronic       0.47      0.08      0.14      1429\n",
      " Hip-Hop/R&B       0.69      0.59      0.64      5593\n",
      "        Jazz       0.54      0.20      0.29      1541\n",
      "         Pop       0.42      0.23      0.30      7887\n",
      "  Rock/Metal       0.66      0.89      0.76     25832\n",
      "\n",
      "   micro avg       0.64      0.64      0.64     45588\n",
      "   macro avg       0.53      0.35      0.38     45588\n",
      "weighted avg       0.60      0.64      0.59     45588\n",
      "\n",
      "\n",
      "Naive Bayes Score:  0.6116083179784154\n",
      "Time Elapsed:  0.299283027649\n",
      "\n",
      "Confusion Matrix:\n",
      "[[    0     2    35    99   170  3000]\n",
      " [    0     0    87    44   109  1189]\n",
      " [    0     1  2966    47   539  2040]\n",
      " [    0     0    27    85   101  1328]\n",
      " [    0     2   498   192  1081  6114]\n",
      " [    0    82   522   441  1037 23750]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Country/Folk       0.00      0.00      0.00      3306\n",
      "  Electronic       0.00      0.00      0.00      1429\n",
      " Hip-Hop/R&B       0.72      0.53      0.61      5593\n",
      "        Jazz       0.09      0.06      0.07      1541\n",
      "         Pop       0.36      0.14      0.20      7887\n",
      "  Rock/Metal       0.63      0.92      0.75     25832\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     45588\n",
      "   macro avg       0.30      0.27      0.27     45588\n",
      "weighted avg       0.51      0.61      0.54     45588\n",
      "\n",
      "\n",
      "K Nearest Neighbor Score:  0.5138632973589541\n",
      "Time Elapsed:  1.86262798309\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  564   115   165   112   326  2024]\n",
      " [  142   105   146    43   151   842]\n",
      " [  289   180  3112    53   482  1477]\n",
      " [  218    71   108   232   141   771]\n",
      " [  866   348   957   235  1336  4145]\n",
      " [ 2545   850  1629   638  2093 18077]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Country/Folk       0.12      0.17      0.14      3306\n",
      "  Electronic       0.06      0.07      0.07      1429\n",
      " Hip-Hop/R&B       0.51      0.56      0.53      5593\n",
      "        Jazz       0.18      0.15      0.16      1541\n",
      "         Pop       0.29      0.17      0.22      7887\n",
      "  Rock/Metal       0.66      0.70      0.68     25832\n",
      "\n",
      "   micro avg       0.51      0.51      0.51     45588\n",
      "   macro avg       0.30      0.30      0.30     45588\n",
      "weighted avg       0.50      0.51      0.51     45588\n",
      "\n",
      "\n",
      "Decision Tree Score:  0.504672282179521\n",
      "Time Elapsed:  1.60474395752\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  548   109   168   143   572  1766]\n",
      " [   97   175   128    32   220   777]\n",
      " [  169   143  2810    90   793  1588]\n",
      " [  135    39    80   376   219   692]\n",
      " [  586   256   778   328  2357  3582]\n",
      " [ 1933   833  1563   893  3869 16741]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Country/Folk       0.16      0.17      0.16      3306\n",
      "  Electronic       0.11      0.12      0.12      1429\n",
      " Hip-Hop/R&B       0.51      0.50      0.51      5593\n",
      "        Jazz       0.20      0.24      0.22      1541\n",
      "         Pop       0.29      0.30      0.30      7887\n",
      "  Rock/Metal       0.67      0.65      0.66     25832\n",
      "\n",
      "   micro avg       0.50      0.50      0.50     45588\n",
      "   macro avg       0.32      0.33      0.33     45588\n",
      "weighted avg       0.51      0.50      0.51     45588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST: Random Forest vs Naive Bayes vs K Nearest Neighbors vs Decision Tree\n",
    "\n",
    "y = df['genre']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RFclassifier = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "startTime = time.time()\n",
    "RFclassifier.fit(X_train, y_train)\n",
    "print \"\\nRandom Forest Score: \", RFclassifier.score(X_test, y_test)\n",
    "print \"Time Elapsed: \", time.time() - startTime\n",
    "preds = RFclassifier.predict(X_test)\n",
    "print \"\\nConfusion Matrix:\"\n",
    "print(sklearn.metrics.confusion_matrix(y_test, preds))\n",
    "print \"\\nClassification Report:\"\n",
    "print sklearn.metrics.classification_report(y_test, preds)\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "NBclassifier = GaussianNB()\n",
    "startTime = time.time()\n",
    "NBclassifier.fit(X_train, y_train)\n",
    "print \"\\nNaive Bayes Score: \", NBclassifier.score(X_test, y_test)\n",
    "print \"Time Elapsed: \", time.time() - startTime\n",
    "preds = NBclassifier.predict(X_test)\n",
    "print \"\\nConfusion Matrix:\"\n",
    "print(sklearn.metrics.confusion_matrix(y_test, preds))\n",
    "print \"\\nClassification Report:\"\n",
    "print sklearn.metrics.classification_report(y_test, preds)\n",
    "\n",
    "\n",
    "# K Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNclassifier = KNeighborsClassifier(n_neighbors=3)\n",
    "startTime = time.time()\n",
    "KNclassifier.fit(X_train, y_train)\n",
    "print \"\\nK Nearest Neighbor Score: \", KNclassifier.score(X_test, y_test)\n",
    "print \"Time Elapsed: \", time.time() - startTime\n",
    "preds = KNclassifier.predict(X_test)\n",
    "print \"\\nConfusion Matrix:\"\n",
    "print(sklearn.metrics.confusion_matrix(y_test, preds))\n",
    "print \"\\nClassification Report:\"\n",
    "print sklearn.metrics.classification_report(y_test, preds)\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DTclassifier = DecisionTreeClassifier(random_state=0)\n",
    "startTime = time.time()\n",
    "DTclassifier.fit(X_train, y_train)\n",
    "print \"\\nDecision Tree Score: \", DTclassifier.score(X_test, y_test)\n",
    "print \"Time Elapsed: \", time.time() - startTime\n",
    "preds = DTclassifier.predict(X_test)\n",
    "print \"\\nConfusion Matrix:\"\n",
    "print(sklearn.metrics.confusion_matrix(y_test, preds))\n",
    "print \"\\nClassification Report:\"\n",
    "print sklearn.metrics.classification_report(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag of Words Model:\n",
      "\n",
      "Score:  0.6964332719136614\n",
      "Time Elapsed:  4555.94374895\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  435     0    15    31    76  2749]\n",
      " [    2   148    67     0    76  1136]\n",
      " [    6     1  3857    19   263  1447]\n",
      " [   25     2    21   360   134   999]\n",
      " [   39    13   274    56  2166  5339]\n",
      " [  104    19   251    70   605 24783]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Country/Folk       0.71      0.13      0.22      3306\n",
      "  Electronic       0.81      0.10      0.18      1429\n",
      " Hip-Hop/R&B       0.86      0.69      0.77      5593\n",
      "        Jazz       0.67      0.23      0.35      1541\n",
      "         Pop       0.65      0.27      0.39      7887\n",
      "  Rock/Metal       0.68      0.96      0.80     25832\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     45588\n",
      "   macro avg       0.73      0.40      0.45     45588\n",
      "weighted avg       0.70      0.70      0.65     45588\n",
      "\n",
      "\n",
      "Sentiment Analysis:\n",
      "\n",
      "Score:  0.5943450030709836\n",
      "Time Elapsed:  43.628526926\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  293    21   118    48   316  2510]\n",
      " [   29   117   101    18   153  1011]\n",
      " [   56    25  2709    35   534  2234]\n",
      " [   53    11    67   304   176   930]\n",
      " [  181    79   510   114  1881  5122]\n",
      " [  547   171  1036   240  2047 21791]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Country/Folk       0.25      0.09      0.13      3306\n",
      "  Electronic       0.28      0.08      0.13      1429\n",
      " Hip-Hop/R&B       0.60      0.48      0.53      5593\n",
      "        Jazz       0.40      0.20      0.26      1541\n",
      "         Pop       0.37      0.24      0.29      7887\n",
      "  Rock/Metal       0.65      0.84      0.73     25832\n",
      "\n",
      "   micro avg       0.59      0.59      0.59     45588\n",
      "   macro avg       0.42      0.32      0.35     45588\n",
      "weighted avg       0.54      0.59      0.55     45588\n",
      "\n",
      "\n",
      "Word Count:\n",
      "\n",
      "Score:  0.6205361059928052\n",
      "Time Elapsed:  12.0014519691\n",
      "\n",
      "Confusion Matrix:\n",
      "[[    0     0    50     0     6  3250]\n",
      " [    0     0   118     0     6  1305]\n",
      " [    1     1  3146     0    80  2365]\n",
      " [    0     0    48     0     5  1488]\n",
      " [    0     0   784     0    62  7041]\n",
      " [    0     1   678     0    72 25081]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Country/Folk       0.00      0.00      0.00      3306\n",
      "  Electronic       0.00      0.00      0.00      1429\n",
      " Hip-Hop/R&B       0.65      0.56      0.60      5593\n",
      "        Jazz       0.00      0.00      0.00      1541\n",
      "         Pop       0.27      0.01      0.02      7887\n",
      "  Rock/Metal       0.62      0.97      0.76     25832\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     45588\n",
      "   macro avg       0.26      0.26      0.23     45588\n",
      "weighted avg       0.48      0.62      0.51     45588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST: Bag of words vs Sentiment Analyis vs Word Count (Uses Random Forest)\n",
    "\n",
    "X1 = n_features\n",
    "X2 = df[['pos_score', 'neg_score', 'neu_score', 'compound_score']].values\n",
    "X3 = df[['word_count']].values\n",
    "y = df['genre']\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# X1: Bag of Words\n",
    "print \"\\nBag of Words Model:\\n\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, train_size=0.8, test_size=0.2, random_state=1)\n",
    "startTime = time.time()\n",
    "classifier.fit(X_train, y_train)\n",
    "print \"Score: \", classifier.score(X_test, y_test)\n",
    "print \"Time Elapsed: \", time.time() - startTime\n",
    "preds = classifier.predict(X_test)\n",
    "print \"\\nConfusion Matrix:\"\n",
    "print(sklearn.metrics.confusion_matrix(y_test, preds))\n",
    "print \"\\nClassification Report:\"\n",
    "print sklearn.metrics.classification_report(y_test, preds)\n",
    "\n",
    "\n",
    "# X2: Sentiment Analysis\n",
    "print \"\\nSentiment Analysis:\\n\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, train_size=0.8, test_size=0.2, random_state=1)\n",
    "startTime = time.time()\n",
    "classifier.fit(X_train, y_train)\n",
    "print \"Score: \", classifier.score(X_test, y_test)\n",
    "print \"Time Elapsed: \", time.time() - startTime\n",
    "preds = classifier.predict(X_test)\n",
    "print \"\\nConfusion Matrix:\"\n",
    "print(sklearn.metrics.confusion_matrix(y_test, preds))\n",
    "print \"\\nClassification Report:\"\n",
    "print sklearn.metrics.classification_report(y_test, preds)\n",
    "\n",
    "\n",
    "# X3: Word Count\n",
    "print \"\\nWord Count:\\n\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X3, y, train_size=0.8, test_size=0.2, random_state=1)\n",
    "startTime = time.time()\n",
    "classifier.fit(X_train, y_train)\n",
    "print \"Score: \", classifier.score(X_test, y_test)\n",
    "print \"Time Elapsed: \", time.time() - startTime\n",
    "preds = classifier.predict(X_test)\n",
    "print \"\\nConfusion Matrix:\"\n",
    "print(sklearn.metrics.confusion_matrix(y_test, preds))\n",
    "print \"\\nClassification Report:\"\n",
    "print sklearn.metrics.classification_report(y_test, preds)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
